{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_MODEL = 128\n",
    "NUM_BLOCK = 1\n",
    "NUM_HEAD = 4\n",
    "DROPOUT = 0.5\n",
    "FE = 1\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 50\n",
    "TEACHER_FORCING_RATE = 0.0\n",
    "LEARNING_RATE = 0.001\n",
    "PATIENCE_THRESHOLD = 4\n",
    "ATTENTION_IMAGE_OUTPUT_PATH = 'image'\n",
    "\n",
    "SMILES_PATH = '../data/ADAGRASIB_UNIQUE_SMILES.txt'\n",
    "COORDINATE_PATH = '../data/ADAGRASIB_COOR.sdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_all_atom(smi_path) :\n",
    "    smi_dic = {}\n",
    "    i = 0\n",
    "\n",
    "    with open(smi_path, 'r') as file :\n",
    "        for smi in file :\n",
    "            smi = rdkit.Chem.MolFromSmiles(smi)\n",
    "            for atom in smi.GetAtoms() :\n",
    "                atom = atom.GetSymbol()\n",
    "                if atom not in smi_dic : \n",
    "                    smi_dic[atom] = i \n",
    "                    i += 1\n",
    "    return smi_dic\n",
    "\n",
    "def replace_atom(input, mode = 'train') :\n",
    "    if mode == 'train' :\n",
    "        smi_list = [smi.replace('Cl', 'X')\n",
    "                    .replace('Br', 'Y')\n",
    "                    .replace('Na', 'Z')\n",
    "                    .replace('Ba', 'T') for smi in input]\n",
    "        return smi_list\n",
    "        \n",
    "    if mode == 'eval' :\n",
    "        smi = input.replace('X', 'Cl').replace('Y', 'Br').replace('Z', 'Na').replace('T', 'Ba').replace('x','')\n",
    "        return smi\n",
    "\n",
    "def get_longest(input_list) :\n",
    "    longest = 0\n",
    "    for i in input_list :\n",
    "        if len(i) > longest :\n",
    "            longest = len(i)\n",
    "    return longest\n",
    "\n",
    "def get_coor(coor_path) :\n",
    "    coor_list = []\n",
    "    supplier = rdkit.Chem.SDMolSupplier(coor_path)\n",
    "    for mol in supplier:\n",
    "        coor = []\n",
    "        if mol is not None:\n",
    "            conformer = mol.GetConformer()\n",
    "            for atom in mol.GetAtoms():\n",
    "                atom_idx = atom.GetIdx()\n",
    "                x, y, z = conformer.GetAtomPosition(atom_idx)\n",
    "                coor_atom = list((x,y,z))\n",
    "                coor.append(coor_atom)\n",
    "        coor_list.append(coor)\n",
    "\n",
    "    # Replace invalid idx\n",
    "    for i, coor in enumerate(coor_list):\n",
    "        \n",
    "        if len(coor) == 0 :\n",
    "            if i == 0 :\n",
    "                coor_list = coor_list[1:]\n",
    "            coor_list[i] = coor_list[i-1]\n",
    "    return coor_list\n",
    "\n",
    "def get_smi(smi_path) :\n",
    "    smi_list = []\n",
    "    i = 0\n",
    "    with open(smi_path, 'r') as file :\n",
    "        for smi in file :\n",
    "            if rdkit.Chem.MolFromSmiles(smi) is None :\n",
    "                if len(smi_list) == 0 :\n",
    "                    continue \n",
    "                smi_list.append(smi_list[i-1])\n",
    "                i += 1 \n",
    "                continue \n",
    "            smi_list.append(smi)\n",
    "            i += 1\n",
    "    \n",
    "    smi_list = [smi[:-1] for smi in smi_list]\n",
    "    smi_list = [smi + 'E' for smi in smi_list]\n",
    "    smi_list = replace_atom(smi_list)\n",
    "    return smi_list\n",
    "\n",
    "def get_dic(smi_list) :\n",
    "    smi_dic = {'x': 0,\n",
    "               'E': 1}\n",
    "    i = len(smi_dic)\n",
    "\n",
    "    for smi in smi_list : \n",
    "        for atom in smi :\n",
    "            if atom not in smi_dic : \n",
    "                smi_dic[atom] = i\n",
    "                i += 1 \n",
    "    return smi_dic \n",
    "\n",
    "def count_atoms(smi):\n",
    "    mol = rdkit.Chem.MolFromSmiles(smi)\n",
    "    if mol is not None:\n",
    "        num_atoms = mol.GetNumAtoms()\n",
    "        return num_atoms\n",
    "    else:\n",
    "        print(\"Error: Unable to parse SMILES string.\")\n",
    "        return None\n",
    "    \n",
    "def smi2int(smi, smi_dic, longest_smi, mode = 'data') :\n",
    "    if mode == 'eval' :\n",
    "        # smi += 'E'\n",
    "        smi = smi + 'E' if smi[-1] != 'E' else smi\n",
    "        smi = list(smi)\n",
    "        smint = [smi_dic[atom] for atom in smi]\n",
    "        smint = smint + [0] * (longest_smi - len(smint))\n",
    "        smint = torch.tensor(smint, dtype=torch.long, device = device)\n",
    "        smint = smint.unsqueeze(0)\n",
    "        return smint\n",
    "    smi = list(smi)\n",
    "    smint = [smi_dic[atom] for atom in smi]\n",
    "    smint = smint + [0] * (longest_smi - len(smint))\n",
    "    return smint \n",
    "\n",
    "def int2smi(smint, inv_smi_dic) :\n",
    "    smint = smint.cpu().numpy()\n",
    "    \n",
    "    smi = [inv_smi_dic[i] for i in smint] \n",
    "    smi = ''.join(smi)\n",
    "    smi = replace_atom(smi, mode = 'eval')\n",
    "    return smi\n",
    "\n",
    "\n",
    "def normalize_coor(coor_list) :\n",
    "    n_coor_list = []\n",
    "\n",
    "    for mol_coor in coor_list :\n",
    "        n_mol_coor = []\n",
    "\n",
    "        x_origin, y_origin, z_origin = mol_coor[0]\n",
    "\n",
    "        for atom_coor in mol_coor :\n",
    "            n_atom_coor = [round(atom_coor[0] - x_origin, 2), \n",
    "                        round(atom_coor[1] - y_origin, 2), \n",
    "                        round(atom_coor[2] - z_origin, 2)]\n",
    "            n_mol_coor.append(n_atom_coor)\n",
    "        n_coor_list.append(n_mol_coor)\n",
    "    return n_coor_list\n",
    "\n",
    "def pad_coor(coor_list, longest_coor) :\n",
    "    p_coor_list = []\n",
    "\n",
    "    for i in coor_list :\n",
    "        if len(i) < longest_coor :\n",
    "            zeros = [[0,0,0]] * (longest_coor - len(i))\n",
    "            zeros = torch.tensor(zeros)\n",
    "            i = torch.tensor(i)\n",
    "            i = torch.cat((i, zeros), dim = 0)\n",
    "            p_coor_list.append(i)\n",
    "        else :\n",
    "            p_coor_list.append(i)\n",
    "    return p_coor_list\n",
    "\n",
    "def split_data(input, ratio = [0.9,0.05,0.05]) :\n",
    "    assert sum(ratio) == 1, \"Ratio does not add up to 1\"  \n",
    "    stop1 = int(len(input) * ratio[0])\n",
    "    stop2 = int(len(input) * (ratio[0] + ratio[1])) \n",
    "\n",
    "    train = input[:stop1]\n",
    "    val = input[stop1:stop2]\n",
    "    test = input[stop2:]\n",
    "\n",
    "    return train, val, test\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def plot_attn(matrix, smi, mode, output_path = \"\", output_name = \"\", output_type = \"show\") :\n",
    "\n",
    "    if mode == \"cross\" :\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(matrix, cmap = \"viridis\")\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticklabels([''] + list(smi))\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "\n",
    "    if mode == \"self\" :\n",
    "        num_head = matrix.shape[0]\n",
    "        \n",
    "        fig, ax = plt.subplots(1, num_head, figsize=(num_head*10,30))\n",
    "        for i, head in enumerate(matrix) :\n",
    "            cax = ax[i].matshow(head, cmap='viridis')\n",
    "            # fig.colorbar(cax)\n",
    "            ax[i].set_xticklabels([''] + list(smi), fontsize=\"xx-large\")\n",
    "            ax[i].set_yticklabels([''] + list(smi), fontsize='xx-large')\n",
    "            \n",
    "            ax[i].xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "            ax[i].yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    if output_type == \"show\" :\n",
    "        plt.show()\n",
    "    if output_type == \"save\" :\n",
    "        plt.savefig(f\"{output_path}/{output_name}\")\n",
    "    plt.close()\n",
    "\n",
    "def evaluate(idx,\n",
    "            encoder, decoder,\n",
    "            inv_smi_dic, smi_list, np_coor_list) :\n",
    "\n",
    "    target_smi = smi_list[idx]\n",
    "    input_coor = np_coor_list[idx]\n",
    "\n",
    "    print(f'Target SMILES: {target_smi[:-1]}')\n",
    "\n",
    "    input_coor = torch.tensor(input_coor, device=device).unsqueeze(0)\n",
    "    with torch.no_grad() : \n",
    "        e_all, h, c, self_attn = encoder(input_coor)\n",
    "        prediction, cross_attn = decoder(e_all, h, c)\n",
    "        _, idx = torch.topk(prediction, 1) \n",
    "        \n",
    "    idx = idx.squeeze(-1).squeeze(0).cpu().numpy()\n",
    "\n",
    "    pred_smi = ''.join([inv_smi_dic[i] for i in idx]).replace('x','')\n",
    "\n",
    "    print(f'Predicted SMILES: {pred_smi}')\n",
    "    return pred_smi, self_attn.squeeze(0), cross_attn.squeeze(0)\n",
    "\n",
    "# def visualize(idx,\n",
    "#               encoder, decoder,\n",
    "#               smi_list, np_coor_list, inv_smi_dic) : \n",
    "#     target_smi = smi_list[idx]\n",
    "#     input_coor = np_coor_list[idx]\n",
    "\n",
    "#     target_smi = target_smi[:-1] if target_smi[-1] == 'E' else target_smi\n",
    "    \n",
    "#     _, self_attn, cross_attn = evaluate(input_coor,\n",
    "#                                         encoder, decoder,\n",
    "#                                         inv_smi_dic)\n",
    "\n",
    "#     smi_len = len(target_smi)\n",
    "#     coor_len = len(input_coor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smi_list = get_smi(SMILES_PATH)\n",
    "smi_dic = get_dic(smi_list)\n",
    "inv_smi_dic = {value:key for key, value in smi_dic.items()}\n",
    "longest_smi = get_longest(smi_list)\n",
    "smint_list = [smi2int(smi, smi_dic, longest_smi) for smi in smi_list]\n",
    "\n",
    "\n",
    "coor_list = get_coor(COORDINATE_PATH)\n",
    "longest_coor = get_longest(coor_list)\n",
    "np_coor_list = pad_coor(normalize_coor(coor_list), longest_coor)\n",
    "\n",
    "\n",
    "train_smint, val_smint, test_smint = split_data(smint_list)\n",
    "train_coor, val_coor, test_coor = split_data(np_coor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coor2SmiDataset(Dataset) :\n",
    "    def __init__(self, coor_list, smint_list) :\n",
    "        self.smint_list = torch.tensor(smint_list, dtype = torch.long, device=device)\n",
    "        self.coor_list = [torch.tensor(coor, device=device) for coor in coor_list]\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.smint_list)\n",
    "    \n",
    "    def __getitem__(self, idx) :\n",
    "        return self.coor_list[idx], self.smint_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_1660\\3110487948.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coor_list = [torch.tensor(coor, device=device) for coor in coor_list]\n"
     ]
    }
   ],
   "source": [
    "B = 16\n",
    "\n",
    "\n",
    "train_set = Coor2SmiDataset(train_coor, train_smint)\n",
    "val_set = Coor2SmiDataset(val_coor, val_smint)\n",
    "test_set = Coor2SmiDataset(test_coor, test_smint)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Attention(nn.Module): # Neural Network Attention \n",
    "    def __init__(self, dim_model):\n",
    "        super(NN_Attention, self).__init__()\n",
    "        self.Wa = nn.Linear(dim_model, dim_model)\n",
    "        self.Ua = nn.Linear(dim_model, dim_model)\n",
    "        self.Va = nn.Linear(dim_model, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        \n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights \n",
    "    \n",
    "\n",
    "class DP_Attention(nn.Module) : # Dot Product Attention\n",
    "    def __init__(self, dim_model, num_head) :\n",
    "        super(DP_Attention, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.dim_head = dim_model // num_head\n",
    "\n",
    "        self.Q = nn.Linear(dim_model, dim_model)\n",
    "        self.K = nn.Linear(dim_model, dim_model)\n",
    "        self.V = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "        self.out = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, Q, K, V) :\n",
    "        B = Q.size(0) # Shape Q, K, V: (B, longest_smi, dim_model)\n",
    "\n",
    "        Q, K, V = self.Q(Q), self.K(K), self.V(V)\n",
    "\n",
    "        len_Q, len_K, len_V = Q.size(1), K.size(1), V.size(1)\n",
    "\n",
    "        Q = Q.reshape(B, self.num_head, len_Q, self.dim_head)\n",
    "        K = K.reshape(B, self.num_head, len_K, self.dim_head)\n",
    "        V = V.reshape(B, self.num_head, len_V, self.dim_head)\n",
    "        \n",
    "        K_T = K.transpose(2,3).contiguous()\n",
    "\n",
    "        attn_score = Q @ K_T\n",
    "\n",
    "        attn_score = attn_score / (self.dim_head ** 1/2)\n",
    "\n",
    "        attn_distribution = torch.softmax(attn_score, dim = -1)\n",
    "\n",
    "        attn = attn_distribution @ V\n",
    "\n",
    "        attn = attn.reshape(B, len_Q, self.num_head * self.dim_head)\n",
    "        \n",
    "        attn = self.out(attn)\n",
    "\n",
    "        return attn, attn_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head, dropout) :\n",
    "        super(Encoder, self).__init__()\n",
    "        self.Self_Attention = DP_Attention(dim_model, num_head) \n",
    "        self.LSTM = nn.LSTM(2 * dim_model, dim_model, batch_first=True)\n",
    "        self.Up_Size = nn.Linear(3, dim_model)\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        x = self.Dropout(self.Up_Size(x))\n",
    "\n",
    "        attn, self_attn = self.Self_Attention(x, x, x) \n",
    "\n",
    "        input_lstm = torch.cat((attn, x), dim = -1)\n",
    "\n",
    "        e_all, (h, c) = self.LSTM(input_lstm)\n",
    "\n",
    "        return e_all, h, c, self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head, output_size, longest_smi, dropout) :\n",
    "        super(Decoder, self).__init__()\n",
    "        self.longest_smi = longest_smi\n",
    "        self.Embedding = nn.Embedding(longest_smi, dim_model)\n",
    "        self.Cross_Attention = NN_Attention(dim_model) \n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "        self.LSTM = nn.LSTM(2 * dim_model, dim_model, batch_first=True)\n",
    "        self.Linear = nn.Linear(dim_model, output_size)\n",
    "    def forward(self, e_all, e_h, e_c, target = None) :\n",
    "        B = e_all.size(0)\n",
    "\n",
    "        d_input = torch.zeros(B, 1, dtype=torch.long, device = device)\n",
    "\n",
    "        d_h, d_c = e_h, e_c \n",
    "\n",
    "        outputs, cross_attn = [], [] \n",
    "\n",
    "        for i in range(self.longest_smi) : \n",
    "            output, d_h, d_c, step_attn = self.forward_step(d_input, d_h, d_c, e_all)\n",
    "\n",
    "            outputs.append(output), cross_attn.append(step_attn)\n",
    "\n",
    "            if target is not None :\n",
    "                d_input = target[:, i].unsqueeze(1)\n",
    "            else : \n",
    "                _, topi = output.topk(1)\n",
    "                d_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        \n",
    "        outputs = torch.cat(outputs, dim = 1)\n",
    "        outputs = F.log_softmax(outputs, dim = -1) \n",
    "\n",
    "        cross_attn = torch.cat(cross_attn, dim = 1)\n",
    "\n",
    "        return outputs, cross_attn\n",
    "\n",
    "    def forward_step(self, d_input, d_h, d_c, e_all) :\n",
    "        embedded = self.Dropout(self.Embedding(d_input))\n",
    "        \n",
    "        query = d_h.permute(1, 0, 2) + d_c.permute(1, 0, 2)\n",
    "\n",
    "        attn, cross_attn = self.Cross_Attention(query, e_all)\n",
    "\n",
    "        input_gru = torch.cat((embedded, attn), dim = 2)\n",
    "\n",
    "        output, (d_h, d_c) = self.LSTM(input_gru, (d_h, d_c)) \n",
    "\n",
    "        output = self.Linear(output) \n",
    "\n",
    "        return output, d_h, d_c, cross_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_loader, val_loader, test_loader,\n",
    "                encoder, decoder,\n",
    "                encoder_optimizer, decoder_optimizer,\n",
    "                criterion, tf):\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "    epoch_test_loss = 0\n",
    "\n",
    "    for input, target in train_loader:\n",
    "        encoder_optimizer.zero_grad(), decoder_optimizer.zero_grad()\n",
    "        \n",
    "        e_all, h, c, self_attn = encoder(input)\n",
    "\n",
    "        # Teacher Forcing\n",
    "        if tf :\n",
    "          prediction, cross_attn = decoder(e_all, h, c, target)\n",
    "        else :\n",
    "          prediction, cross_attn = decoder(e_all, h, c)\n",
    "\n",
    "\n",
    "        loss = criterion(\n",
    "           prediction.view(-1, prediction.size(-1)),\n",
    "           target.view(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step(), decoder_optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "\n",
    "    encoder.eval(), decoder.eval()\n",
    "\n",
    "    with torch.no_grad() :\n",
    "      for input, target in val_loader :\n",
    "        e_all, h, c, self_attn = encoder(input)\n",
    "        prediction, cross_attn = decoder(e_all, h, c)\n",
    "\n",
    "        test_loss = criterion(\n",
    "           prediction.view(-1, prediction.size(-1)),\n",
    "           target.view(-1)\n",
    "        )\n",
    "        epoch_test_loss += test_loss.item()\n",
    "\n",
    "    return epoch_train_loss / len(train_loader), epoch_test_loss / len(val_loader)\n",
    "\n",
    "def train(train_loader, val_loader, test_loader,\n",
    "          encoder, decoder, \n",
    "          patience_threshold, num_epoch=50, learning_rate=0.001, tf_rate = 0):\n",
    "    start = time.time()\n",
    "    \n",
    "    best_val = float('inf')\n",
    "    patience = 0\n",
    "\n",
    "    train_plot, val_plot = [], []\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    tf = True\n",
    "\n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "      if epoch > (tf_rate * num_epoch) :\n",
    "        tf = False\n",
    "      encoder.train()\n",
    "      decoder.train()\n",
    "\n",
    "      train_loss, val_loss = train_epoch(train_loader,val_loader, test_loader,\n",
    "                                         encoder, decoder,\n",
    "                                         encoder_optimizer, decoder_optimizer,\n",
    "                                         criterion, tf)\n",
    "\n",
    "      print('%s (%d %d%%) /// Train Loss: %.4f - Validation Loss: %.4f' % (timeSince(start, epoch / num_epoch),\n",
    "                                      epoch, epoch / num_epoch * 100, train_loss, val_loss))\n",
    "\n",
    "      if val_loss < best_val :\n",
    "        best_val = val_loss\n",
    "        patience = 0\n",
    "      else :\n",
    "        patience += 1 \n",
    "      \n",
    "      if patience > patience_threshold : \n",
    "        print(\"EARLY STOPPING !!!\")\n",
    "        plt.plot(x, train_plot, color = 'blue', label = 'Train Loss')\n",
    "        plt.plot(x, val_plot, color = 'red', label = 'Validation Loss')\n",
    "        plt.title(\"Final Plot Before Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "      train_plot.append(train_loss), val_plot.append(val_loss)\n",
    "      x = np.linspace(0, num_epoch, epoch)\n",
    "\n",
    "      if epoch == 1 : \n",
    "        continue\n",
    "      if epoch % 5 == 0 :\n",
    "        plt.plot(x, train_plot, color = 'blue', label = 'Train Loss')\n",
    "        plt.plot(x, val_plot, color = 'red', label = 'Validation Loss')\n",
    "        plt.title(f'Epoch {epoch}')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(DIM_MODEL, NUM_HEAD, DROPOUT).to(device)\n",
    "decoder = Decoder(DIM_MODEL, NUM_HEAD, len(smi_dic),longest_smi, DROPOUT).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 1 required positional argument: 'patience_threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m      \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtf_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTEACHER_FORCING_RATE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'patience_threshold'"
     ]
    }
   ],
   "source": [
    "train(train_loader, val_loader, test_loader,\n",
    "      encoder, decoder,\n",
    "      num_epoch=NUM_EPOCHS,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      patience_threshold=PATIENCE_THRESHOLD,\n",
    "      tf_rate = TEACHER_FORCING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(idx,\n",
    "            encoder, decoder,\n",
    "            inv_smi_dic, smi_list, np_coor_list) :\n",
    "\n",
    "    target_smi = smi_list[idx]\n",
    "    input_coor = np_coor_list[idx]\n",
    "\n",
    "\n",
    "    input_coor = torch.tensor(input_coor, device=device).unsqueeze(0)\n",
    "    with torch.no_grad() : \n",
    "        e_all, h, c, self_attn = encoder(input_coor)\n",
    "        prediction, cross_attn = decoder(e_all, h, c)\n",
    "        _, idx = torch.topk(prediction, 1) \n",
    "        \n",
    "    idx = idx.squeeze(-1).squeeze(0).cpu().numpy()\n",
    "\n",
    "    pred_smi = ''.join([inv_smi_dic[i] for i in idx]).replace('x','')\n",
    "\n",
    "    \n",
    "    print(f'Target SMILES: {target_smi[:-1]}')\n",
    "    print(f'Predicted SMILES: {pred_smi}')\n",
    "\n",
    "    \n",
    "    return pred_smi, self_attn.squeeze(0), cross_attn.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target SMILES: CC(C)(CCC=C)C1NC(=O)NC1=O\n",
      "Predicted SMILES: \\TNN---.-3#KPP13NNNNNK/S--#++KKS/S-33NKK#B1+KST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_1660\\3026167585.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_coor = torch.tensor(input_coor, device=device).unsqueeze(0)\n"
     ]
    }
   ],
   "source": [
    "r = random.randint(0, len(smi_list))\n",
    "out = evaluate(r, encoder, decoder, inv_smi_dic, smi_list,np_coor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
