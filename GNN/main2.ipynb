{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch_geometric\n",
    "from torch_geometric.data import Dataset, Data \n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.nn as gnn\n",
    "import rdkit\n",
    "from rdkit.Chem import MolFromSmiles as get_mol\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smi(path) :\n",
    "    with open(path, 'r') as file :\n",
    "        contents = file.readlines() \n",
    "    smi_list = [content[:-1] for content in contents]\n",
    "    return smi_list\n",
    "\n",
    "def get_coor(path) :\n",
    "    coor_list = []\n",
    "    supplier = rdkit.Chem.SDMolSupplier(path)\n",
    "    for mol in supplier:\n",
    "        coor = []\n",
    "        if mol is not None:\n",
    "            conformer = mol.GetConformer()\n",
    "            for atom in mol.GetAtoms():\n",
    "                atom_idx = atom.GetIdx()\n",
    "                x, y, z = conformer.GetAtomPosition(atom_idx)\n",
    "                coor_atom = list((x,y,z))\n",
    "                coor.append(coor_atom)\n",
    "        coor_list.append(coor)\n",
    "\n",
    "    # Replace invalid idx\n",
    "    for i, coor in enumerate(coor_list):\n",
    "        \n",
    "        if len(coor) == 0 :\n",
    "            if i == 0 :\n",
    "                coor_list = coor_list[1:]\n",
    "            coor_list[i] = coor_list[i-1]\n",
    "    return coor_list\n",
    "\n",
    "def get_edge_index(mol) :\n",
    "    edge_indices, begin, end = [], [], []\n",
    "\n",
    "    for bond in mol.GetBonds() :\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx() \n",
    "        begin.append(i), end.append(j), begin.append(j), end.append(i)\n",
    "\n",
    "        # end.append(i), end.append(j)\n",
    "    edge_indices.append(begin), edge_indices.append(end)\n",
    "\n",
    "    return torch.tensor(edge_indices)\n",
    "\n",
    "def get_node_features(mol) :\n",
    "    all_node_feats = [] \n",
    "\n",
    "    for atom in mol.GetAtoms() :\n",
    "        node_feats = []\n",
    "        node_feats.append(atom.GetAtomicNum())\n",
    "        node_feats.append(atom.GetDegree())\n",
    "        node_feats.append(atom.GetFormalCharge())\n",
    "        node_feats.append(atom.GetHybridization())\n",
    "        node_feats.append(atom.GetIsAromatic())\n",
    "        node_feats.append(atom.GetTotalNumHs())\n",
    "        node_feats.append(atom.GetNumRadicalElectrons())\n",
    "        node_feats.append(atom.IsInRing())\n",
    "        node_feats.append(atom.GetChiralTag())\n",
    "\n",
    "        all_node_feats.append(node_feats)\n",
    "\n",
    "    all_node_feats = np.asarray(all_node_feats)\n",
    "    return torch.tensor(all_node_feats, dtype=torch.float)\n",
    "\n",
    "def count_atoms(smi):\n",
    "    mol = rdkit.Chem.MolFromSmiles(smi)\n",
    "    if mol is not None:\n",
    "        num_atoms = mol.GetNumAtoms()\n",
    "        return num_atoms\n",
    "    else:\n",
    "        print(\"Error: Unable to parse SMILES string.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smi_list = get_smi('./data/ADAGRASIB_SMILES.txt')\n",
    "coor_list = get_coor('./data/ADAGRASIB_COOR.sdf')\n",
    "mol_list = [get_mol(smi) for smi in smi_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_idx_list = [get_edge_index(mol) for mol in mol_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feat_list = [get_node_features(mol) for mol in mol_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset) : \n",
    "    def __init__(self, root, filename, transform = None, pre_transform = None) :\n",
    "        self.filename = filename \n",
    "        super(MyDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) :\n",
    "        return self.filename \n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) :\n",
    "        self.smi_list = get_smi(self.raw_paths[0])\n",
    "        return [f'data_{i}.pt' for i in range(len(self.smi_list))]\n",
    "    \n",
    "    def download(self) : pass \n",
    "\n",
    "    def process(self) :\n",
    "        smi_list = get_smi(self.raw_paths[0])\n",
    "        # mol_list = [get_mol(smi) for smi in smi_list]\n",
    "        coor_list = get_coor(self.raw_paths[1])\n",
    "\n",
    "        for i, smi in enumerate(tqdm(smi_list, total=len(smi_list))) :\n",
    "            if count_atoms(smi) != len(coor_list[i]) :\n",
    "                smi = smi_list[i - 69]\n",
    "                print('true')\n",
    "            mol = get_mol(smi) \n",
    "\n",
    "            node_feat = get_node_features(mol)\n",
    "            edge_i = get_edge_index(mol)\n",
    "\n",
    "            data = Data(x = node_feat,\n",
    "                        edge_index = edge_i,\n",
    "                        y = torch.tensor(coor_list[i]))\n",
    "            torch.save(data, os.path.join(self.processed_dir, f'data_{i}.pt'))\n",
    "\n",
    "\n",
    "    def len(self) :\n",
    "        return len(self.smi_list)\n",
    "    \n",
    "    def get(self, idx) :\n",
    "        data = torch.load(os.path.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw/ADAGRASIB_SMILES.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mMyDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mADAGRASIB_SMILES.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mADAGRASIB_COOR.sdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [6], line 4\u001b[0m, in \u001b[0;36mMyDataset.__init__\u001b[0;34m(self, root, filename, transform, pre_transform)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root, filename, transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, pre_transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) :\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m=\u001b[39m filename \n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMyDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/dataset.py:102\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_process:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/dataset.py:228\u001b[0m, in \u001b[0;36mDataset._process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m osp\u001b[38;5;241m.\u001b[39mexists(f) \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f) \u001b[38;5;241m!=\u001b[39m _repr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_filter):\n\u001b[1;32m    222\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `pre_filter` argument differs from the one used in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe pre-processed version of this dataset. If you want to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake use of another pre-fitering technique, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelete \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{self.processed_dir}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m first\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m files_exist(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_paths\u001b[49m):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytest\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/dataset.py:187\u001b[0m, in \u001b[0;36mDataset.processed_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocessed_paths\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The absolute filepaths that must be present in order to skip\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    processing.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_file_names\u001b[49m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# Prevent a common source of error in which `file_names` are not\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# defined as a property.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(files, Callable):\n",
      "Cell \u001b[0;32mIn [6], line 12\u001b[0m, in \u001b[0;36mMyDataset.processed_file_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocessed_file_names\u001b[39m(\u001b[38;5;28mself\u001b[39m) :\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmi_list \u001b[38;5;241m=\u001b[39m \u001b[43mget_smi\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmi_list))]\n",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m, in \u001b[0;36mget_smi\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_smi\u001b[39m(path) :\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file :\n\u001b[1;32m      3\u001b[0m         contents \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines() \n\u001b[1;32m      4\u001b[0m     smi_list \u001b[38;5;241m=\u001b[39m [content[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m content \u001b[38;5;129;01min\u001b[39;00m contents]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw/ADAGRASIB_SMILES.txt'"
     ]
    }
   ],
   "source": [
    "train_set = MyDataset(root='data', filename=['ADAGRASIB_SMILES.txt', 'ADAGRASIB_COOR.sdf'])\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [6., 2., 0.,  ..., 0., 0., 0.],\n",
      "        [6., 2., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [6., 2., 0.,  ..., 0., 1., 0.],\n",
      "        [7., 2., 0.,  ..., 0., 1., 0.],\n",
      "        [6., 2., 0.,  ..., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader :\n",
    "    print(i.\\)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head, dropout) :\n",
    "        super(Model, self).__init__() \n",
    "\n",
    "        self.GATConv1 = gnn.GATConv(train_set.num_node_features, dim_model, heads=num_head, dropout=dropout)\n",
    "        self.GATConv2 = gnn.GATConv(dim_model * num_head, 3, dropout=dropout)\n",
    "\n",
    "    def forward(self, input) :\n",
    "        x, edge_idx, batch = input.x, input.edge_index, input.batch \n",
    "\n",
    "        x = self.GATConv1(x, edge_idx)\n",
    "        x = F.leaky_relu(x) \n",
    "        x = self.GATConv2(x, edge_idx)\n",
    "        x = F.leaky_relu(x) \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(128, 2, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.727, -1.0768, -0.0018], [-2.7591, -0.482, -0.0015], [-1.5391, 0.2678, -0.0011], [-1.5733, 1.6642, -0.0002], [-0.3949, 2.38, 0.0006], [0.8217, 1.7192, 0.0006], [0.866, 0.3376, -0.0007], [-0.3071, -0.3944, 0.0043], [-0.2635, -1.7447, 0.0031], [2.1717, -0.3595, -0.0012], [3.2058, 0.2841, -0.0004], [2.2121, -1.5769, -0.0023]]\n"
     ]
    }
   ],
   "source": [
    "for input in train_loader :\n",
    "    print(input.y[0]);break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
