{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Method***\n",
    "\n",
    "- Implement a Transformer based Seq2Seq architecture. The original Transformer used positional encoding to process the all the input token at the same time. My model use Recurrent Neural Network for both processing the input and output. Each character of SMILES string will be first mapped to integer, them passed through Embedding layer onto higher dimension and finally passed to the Encoder.\n",
    "\n",
    "- Encoder: Input will be first passed to Self Attention layer to let the model learn the relationship between atoms and bonds in the SMILES. The output attention will be concatenate with the original input to pass to an LSTM layer. LSTM will process the input sequentially and output all hidden states at each time steps and the last state which contain all necessary information of the input. All hidden states and last state will be passed to the Decoder.\n",
    "\n",
    "- Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_MODEL = 128\n",
    "NUM_BLOCK = 1\n",
    "NUM_HEAD = 4\n",
    "DROPOUT = 0.5\n",
    "FE = 1\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 50\n",
    "TEACHER_FORCING_RATE = 0.0\n",
    "LEARNING_RATE = 0.001\n",
    "PATIENCE_THRESHOLD = 4\n",
    "ATTENTION_IMAGE_OUTPUT_PATH = 'image'\n",
    "\n",
    "SMILES_DATA_PATH = '../data/ADAGRASIB_UNIQUE_SMILES.txt'\n",
    "COORDINATE_DATA_PATH = '../data/ADAGRASIB_COOR.sdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_all_atom(smi_path) :\n",
    "    smi_dic = {}\n",
    "    i = 0\n",
    "\n",
    "    with open(smi_path, 'r') as file :\n",
    "        for smi in file :\n",
    "            smi = rdkit.Chem.MolFromSmiles(smi)\n",
    "            for atom in smi.GetAtoms() :\n",
    "                atom = atom.GetSymbol()\n",
    "                if atom not in smi_dic : \n",
    "                    smi_dic[atom] = i \n",
    "                    i += 1\n",
    "    return smi_dic\n",
    "\n",
    "def replace_atom(input, mode = 'train') :\n",
    "    if mode == 'train' :\n",
    "        smi_list = [smi.replace('Cl', 'X')\n",
    "                    .replace('Br', 'Y')\n",
    "                    .replace('Na', 'Z')\n",
    "                    .replace('Ba', 'T') for smi in input]\n",
    "        return smi_list\n",
    "        \n",
    "    if mode == 'eval' :\n",
    "        smi = input.replace('X', 'Cl').replace('Y', 'Br').replace('Z', 'Na').replace('T', 'Ba').replace('x','')\n",
    "        return smi\n",
    "\n",
    "def get_longest(input_list) :\n",
    "    longest = 0\n",
    "    for i in input_list :\n",
    "        if len(i) > longest :\n",
    "            longest = len(i)\n",
    "    return longest\n",
    "\n",
    "def get_coor(coor_path) :\n",
    "    coor_list = []\n",
    "    supplier = rdkit.Chem.SDMolSupplier(coor_path)\n",
    "    for mol in supplier:\n",
    "        coor = []\n",
    "        if mol is not None:\n",
    "            conformer = mol.GetConformer()\n",
    "            for atom in mol.GetAtoms():\n",
    "                atom_idx = atom.GetIdx()\n",
    "                x, y, z = conformer.GetAtomPosition(atom_idx)\n",
    "                coor_atom = list((x,y,z))\n",
    "                coor.append(coor_atom)\n",
    "        coor_list.append(coor)\n",
    "\n",
    "    # Replace invalid idx\n",
    "    for i, coor in enumerate(coor_list):\n",
    "        \n",
    "        if len(coor) == 0 :\n",
    "            if i == 0 :\n",
    "                coor_list = coor_list[1:]\n",
    "            coor_list[i] = coor_list[i-1]\n",
    "    return coor_list\n",
    "\n",
    "def get_smi(smi_path) :\n",
    "    smi_list = []\n",
    "    i = 0\n",
    "    with open(smi_path, 'r') as file :\n",
    "        for smi in file :\n",
    "            if rdkit.Chem.MolFromSmiles(smi) is None :\n",
    "                if len(smi_list) == 0 :\n",
    "                    continue \n",
    "                smi_list.append(smi_list[i-1])\n",
    "                i += 1 \n",
    "                continue \n",
    "            smi_list.append(smi)\n",
    "            i += 1\n",
    "    \n",
    "    smi_list = [smi[:-1] for smi in smi_list]\n",
    "    smi_list = [smi + 'E' for smi in smi_list]\n",
    "    smi_list = replace_atom(smi_list)\n",
    "    return smi_list\n",
    "\n",
    "def get_dic(smi_list) :\n",
    "    smi_dic = {'x': 0,\n",
    "               'E': 1}\n",
    "    i = len(smi_dic)\n",
    "\n",
    "    for smi in smi_list : \n",
    "        for atom in smi :\n",
    "            if atom not in smi_dic : \n",
    "                smi_dic[atom] = i\n",
    "                i += 1 \n",
    "    return smi_dic \n",
    "\n",
    "def count_atoms(smi):\n",
    "    mol = rdkit.Chem.MolFromSmiles(smi)\n",
    "    if mol is not None:\n",
    "        num_atoms = mol.GetNumAtoms()\n",
    "        return num_atoms\n",
    "    else:\n",
    "        print(\"Error: Unable to parse SMILES string.\")\n",
    "        return None\n",
    "    \n",
    "def smi2int(smi, smi_dic, longest_smi, mode = 'data') :\n",
    "    if mode == 'eval' :\n",
    "        # smi += 'E'\n",
    "        smi = smi + 'E' if smi[-1] != 'E' else smi\n",
    "        smi = list(smi)\n",
    "        smint = [smi_dic[atom] for atom in smi]\n",
    "        smint = smint + [0] * (longest_smi - len(smint))\n",
    "        smint = torch.tensor(smint, dtype=torch.long, device = device)\n",
    "        smint = smint.unsqueeze(0)\n",
    "        return smint\n",
    "    smi = list(smi)\n",
    "    smint = [smi_dic[atom] for atom in smi]\n",
    "    smint = smint + [0] * (longest_smi - len(smint))\n",
    "    return smint \n",
    "\n",
    "def int2smi(smint, inv_smi_dic) :\n",
    "    smint = smint.cpu().numpy()\n",
    "    \n",
    "    smi = [inv_smi_dic[i] for i in smint] \n",
    "    smi = ''.join(smi)\n",
    "    smi = replace_atom(smi, mode = 'eval')\n",
    "    return smi\n",
    "\n",
    "\n",
    "def normalize_coor(coor_list) :\n",
    "    n_coor_list = []\n",
    "\n",
    "    for mol_coor in coor_list :\n",
    "        n_mol_coor = []\n",
    "\n",
    "        x_origin, y_origin, z_origin = mol_coor[0]\n",
    "\n",
    "        for atom_coor in mol_coor :\n",
    "            n_atom_coor = [round(atom_coor[0] - x_origin, 2), \n",
    "                        round(atom_coor[1] - y_origin, 2), \n",
    "                        round(atom_coor[2] - z_origin, 2)]\n",
    "            n_mol_coor.append(n_atom_coor)\n",
    "        n_coor_list.append(n_mol_coor)\n",
    "    return n_coor_list\n",
    "\n",
    "def pad_coor(coor_list, longest_coor) :\n",
    "    p_coor_list = []\n",
    "\n",
    "    for i in coor_list :\n",
    "        if len(i) < longest_coor :\n",
    "            zeros = [[0,0,0]] * (longest_coor - len(i))\n",
    "            zeros = torch.tensor(zeros)\n",
    "            i = torch.tensor(i)\n",
    "            i = torch.cat((i, zeros), dim = 0)\n",
    "            p_coor_list.append(i)\n",
    "        else :\n",
    "            p_coor_list.append(i)\n",
    "    return p_coor_list\n",
    "\n",
    "def split_data(input, ratio = [0.9,0.05,0.05]) :\n",
    "    assert sum(ratio) == 1, \"Ratio does not add up to 1\"  \n",
    "    stop1 = int(len(input) * ratio[0])\n",
    "    stop2 = int(len(input) * (ratio[0] + ratio[1])) \n",
    "\n",
    "    train = input[:stop1]\n",
    "    val = input[stop1:stop2]\n",
    "    test = input[stop2:]\n",
    "\n",
    "    return train, val, test\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def evaluate(input, encoder, decoder, smi_dic, longest_smi) :\n",
    "    encoder.eval(), decoder.eval()\n",
    "    if type(input) == str :\n",
    "        input = smi2int(input, smi_dic, longest_smi, mode = 'eval')\n",
    "    \n",
    "    with torch.no_grad() :\n",
    "        e_all, e_last, self_attn = encoder(input) \n",
    "        prediction, cross_attn = decoder(e_all, e_last)\n",
    "\n",
    "        prediction, self_attn, cross_attn = prediction.squeeze(0), self_attn.squeeze(0), cross_attn.squeeze(0)\n",
    "    return prediction, self_attn, cross_attn\n",
    "\n",
    "\n",
    "def plot_attn(matrix, smi, mode, output_path = \"\", output_name = \"\", output_type = \"show\") :\n",
    "\n",
    "    if mode == \"cross\" :\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(matrix, cmap = \"viridis\")\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticklabels([''] + list(smi))\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "\n",
    "    if mode == \"self\" :\n",
    "        num_head = matrix.shape[0]\n",
    "        \n",
    "        fig, ax = plt.subplots(1, num_head, figsize=(num_head*10,30))\n",
    "        for i, head in enumerate(matrix) :\n",
    "            cax = ax[i].matshow(head, cmap='viridis')\n",
    "            # fig.colorbar(cax)\n",
    "            ax[i].set_xticklabels([''] + list(smi), fontsize=\"xx-large\")\n",
    "            ax[i].set_yticklabels([''] + list(smi), fontsize='xx-large')\n",
    "            \n",
    "            ax[i].xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "            ax[i].yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    if output_type == \"show\" :\n",
    "        plt.show()\n",
    "    if output_type == \"save\" :\n",
    "        plt.savefig(f\"{output_path}/{output_name}\")\n",
    "    plt.close()\n",
    "    \n",
    "def visualize(smi,\n",
    "              encoder, decoder,\n",
    "              smi_dic, longest_smi,\n",
    "              output_path = ATTENTION_IMAGE_OUTPUT_PATH,\n",
    "              output_name=\"test\",\n",
    "              output_type=\"show\") :\n",
    "    \n",
    "    \n",
    "    prediction, self_attn, cross_attn = evaluate(smi, encoder, decoder, smi_dic, longest_smi)\n",
    "\n",
    "    self_attn = self_attn.cpu().numpy()\n",
    "    cross_attn = cross_attn.cpu().numpy()\n",
    "\n",
    "    smi = smi[:-1] if smi[-1] == 'E' else smi\n",
    "    smi = replace_atom(smi, mode='eval')\n",
    "\n",
    "    coor_len = count_atoms(smi)\n",
    "    smi_len = len(smi) \n",
    "\n",
    "    plot_attn(self_attn[:, :smi_len, :smi_len],smi=smi, mode='self', output_path=output_path, output_name=f\"{output_name}-SELF\", output_type=output_type)\n",
    "    plot_attn(cross_attn[:coor_len, :smi_len],smi=smi, mode='cross', output_path=output_path, output_name=f\"{output_name}-CROSS\", output_type=output_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "smi_list = get_smi(SMILES_DATA_PATH)\n",
    "longest_smi = get_longest(smi_list)\n",
    "smi_dic = get_dic(smi_list)\n",
    "inv_smi_dic = {value:key for key, value in smi_dic.items()}\n",
    "smint_list = [smi2int(smi, smi_dic, longest_smi) for smi in smi_list]\n",
    "\n",
    "\n",
    "coor_list = get_coor(COORDINATE_DATA_PATH)\n",
    "longest_coor = get_longest(coor_list)\n",
    "np_coor_list = pad_coor(normalize_coor(coor_list), longest_coor)\n",
    "\n",
    "\n",
    "train_smint, val_smint, test_smint = split_data(smint_list)\n",
    "train_coor, val_coor, test_coor = split_data(np_coor_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_15076\\3954692349.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coor_list = [torch.tensor(coor, device=device) for coor in coor_list]\n"
     ]
    }
   ],
   "source": [
    "class ADAGRASIB_Dataset(Dataset) :\n",
    "    def __init__(self, smi_list, coor_list) :\n",
    "        self.smi_list = torch.tensor(smi_list, dtype = torch.long, device=device)\n",
    "        self.coor_list = [torch.tensor(coor, device=device) for coor in coor_list]\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.smi_list)\n",
    "    \n",
    "    def __getitem__(self, idx) :\n",
    "        return self.smi_list[idx], self.coor_list[idx]\n",
    "    \n",
    "train_set = ADAGRASIB_Dataset(train_smint, train_coor)\n",
    "val_set = ADAGRASIB_Dataset(val_smint, val_coor)\n",
    "test_set = ADAGRASIB_Dataset(test_smint, test_coor)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Attention(nn.Module): # Neural Network Attention \n",
    "    def __init__(self, dim_model):\n",
    "        super(NN_Attention, self).__init__()\n",
    "        self.Wa = nn.Linear(dim_model, dim_model)\n",
    "        self.Ua = nn.Linear(dim_model, dim_model)\n",
    "        self.Va = nn.Linear(dim_model, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        \n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights \n",
    "    \n",
    "\n",
    "class DP_Attention(nn.Module) : # Dot Product Attention\n",
    "    def __init__(self, dim_model, num_head) :\n",
    "        super(DP_Attention, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.dim_head = dim_model // num_head\n",
    "\n",
    "        self.Q = nn.Linear(dim_model, dim_model)\n",
    "        self.K = nn.Linear(dim_model, dim_model)\n",
    "        self.V = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "        self.out = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, Q, K, V) :\n",
    "        B = Q.size(0) # Shape Q, K, V: (B, longest_smi, dim_model)\n",
    "\n",
    "        Q, K, V = self.Q(Q), self.K(K), self.V(V)\n",
    "\n",
    "        len_Q, len_K, len_V = Q.size(1), K.size(1), V.size(1)\n",
    "\n",
    "        Q = Q.reshape(B, self.num_head, len_Q, self.dim_head)\n",
    "        K = K.reshape(B, self.num_head, len_K, self.dim_head)\n",
    "        V = V.reshape(B, self.num_head, len_V, self.dim_head)\n",
    "        \n",
    "        K_T = K.transpose(2,3).contiguous()\n",
    "\n",
    "        attn_score = Q @ K_T\n",
    "\n",
    "        attn_score = attn_score / (self.dim_head ** 1/2)\n",
    "\n",
    "        attn_distribution = torch.softmax(attn_score, dim = -1)\n",
    "\n",
    "        attn = attn_distribution @ V\n",
    "\n",
    "        attn = attn.reshape(B, len_Q, self.num_head * self.dim_head)\n",
    "        \n",
    "        attn = self.out(attn)\n",
    "\n",
    "        return attn, attn_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head, fe, dropout) :\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.Self_Attention = DP_Attention(dim_model,num_head)\n",
    "        self.LSTM = nn.LSTM(input_size=2 * dim_model, hidden_size=dim_model, batch_first=True)\n",
    "\n",
    "    def forward(self, Q, K, V) :\n",
    "\n",
    "        attn, self_attn = self.Self_Attention(Q, Q, Q)\n",
    "\n",
    "        input_lstm = torch.cat((Q, attn), dim = -1)\n",
    "\n",
    "        all_state, (last_state, _) = self.LSTM(input_lstm)\n",
    "\n",
    "        return all_state, last_state, self_attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module) :\n",
    "    def __init__(self, dim_model, num_block, num_head, len_dic, fe = 1, dropout = 0.1) :\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.Embedding = nn.Embedding(len_dic, dim_model)\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            EncoderBlock(dim_model, num_head, fe, dropout) for _ in range(num_block)\n",
    "        )\n",
    "\n",
    "    def forward(self, x) :\n",
    "        out = self.Dropout(self.Embedding(x))\n",
    "        for block in self.encoder_blocks : \n",
    "            out, last_state, self_attn = block(out, out, out) \n",
    "        return out, last_state, self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module) :\n",
    "    def __init__(self, dim_model, longest_coor,dropout, num_head = 1, output_size = 3) :\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.longest_coor = longest_coor\n",
    "\n",
    "        self.cross_attn_nn = NN_Attention(dim_model)\n",
    "\n",
    "        self.gru = nn.GRU(3 + dim_model, dim_model, batch_first=True)\n",
    "\n",
    "        self.out = nn.Linear(dim_model, output_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, e_all, e_last, target = None) :\n",
    "        B = e_all.size(0)\n",
    "\n",
    "        d_input = torch.zeros(B, 1, 3).to(device)\n",
    "\n",
    "        d_hidden = e_last\n",
    "\n",
    "        d_outputs, cross_attn = [], []\n",
    "\n",
    "        for i in range(self.longest_coor) :\n",
    "            d_output, d_hidden, step_attn = self.forward_step(d_input, d_hidden, e_all)\n",
    "\n",
    "            d_outputs.append(d_output), cross_attn.append(step_attn)\n",
    "\n",
    "            if target is not None :\n",
    "                d_input = target[:, i, :].unsqueeze(1)\n",
    "            else :\n",
    "                d_input = d_output\n",
    "\n",
    "        d_outputs = torch.cat(d_outputs, dim = 1)\n",
    "\n",
    "        cross_attn = torch.cat(cross_attn, dim = 1)\n",
    "        \n",
    "        return d_outputs, d_hidden, cross_attn\n",
    "\n",
    "\n",
    "    def forward_step(self, d_input, d_hidden, e_all) :\n",
    "        Q = d_hidden.permute(1,0,2)\n",
    "\n",
    "        d_input = self.dropout(d_input)\n",
    "\n",
    "        attn, attn_distribution = self.cross_attn_nn(Q, e_all)\n",
    "\n",
    "        input_lstm = torch.cat((attn, d_input), dim = 2)\n",
    "\n",
    "        output, d_hidden = self.gru(input_lstm, d_hidden) \n",
    "        \n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, d_hidden, attn_distribution\n",
    "    \n",
    "\n",
    "class DecoderBlock(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head, longest_coor, fe, dropout) :\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.gru = GRU(dim_model, longest_coor,dropout, num_head)\n",
    "\n",
    "\n",
    "    def forward(self, e_all, e_last, target = None) :\n",
    "        output, _, cross_attn = self.gru(e_all, e_last, target)\n",
    "        \n",
    "        return output, cross_attn\n",
    "    \n",
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, dim_model,num_block, num_head, longest_coor, fe = 1, dropout = 0.1) :\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [DecoderBlock(dim_model, num_head,longest_coor, fe, dropout) for _ in range(num_block)]\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, e_all, e_last, target = None) :\n",
    "        for block in self.decoder_blocks :\n",
    "            target, cross_attn = block(e_all, e_last, target)\n",
    "        return target, cross_attn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = random.randint(0, len(smi_list))\n",
    "r2 = random.randint(0, len(smi_list))\n",
    "r3 = random.randint(0, len(smi_list))\n",
    "\n",
    "def train_epoch(train_loader, val_loader, test_loader,\n",
    "                encoder, decoder,\n",
    "                encoder_optimizer, decoder_optimizer,\n",
    "                criterion, tf):\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "    epoch_val_loss = 0\n",
    "\n",
    "    for input, target in train_loader:\n",
    "\n",
    "        encoder_optimizer.zero_grad(), decoder_optimizer.zero_grad()\n",
    "        \n",
    "        e_all, e_last, self_attn = encoder(input)\n",
    "\n",
    "        # Teacher Forcing\n",
    "        if tf :\n",
    "          prediction, cross_attn = decoder(e_all, e_last, target)\n",
    "        else :\n",
    "          prediction, cross_attn = decoder(e_all, e_last)\n",
    "\n",
    "\n",
    "        loss = criterion(prediction, target)\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step(), decoder_optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "\n",
    "    encoder.eval(), decoder.eval()\n",
    "    \n",
    "\n",
    "\n",
    "    with torch.no_grad() :\n",
    "      for input, target in val_loader :\n",
    "        e_all, e_last, self_attn = encoder(input)\n",
    "        prediction, cross_attn = decoder(e_all, e_last)\n",
    "\n",
    "        test_loss = criterion(prediction, target)\n",
    "        epoch_val_loss += test_loss.item()\n",
    "\n",
    "    return epoch_train_loss / len(train_loader), epoch_val_loss / len(test_loader)\n",
    "\n",
    "def train(train_loader,val_loader, test_loader,\n",
    "          encoder, decoder, \n",
    "          num_epoch=50,\n",
    "          learning_rate=0.001, tf_rate = 1):\n",
    "    start = time.time()\n",
    "    \n",
    "    best_val = float('inf')\n",
    "    patience = 0\n",
    "\n",
    "    train_plot, val_plot = [], []\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    tf = True\n",
    "\n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "      if epoch > (tf_rate * num_epoch) :\n",
    "        tf = False\n",
    "      encoder.train()\n",
    "      decoder.train()\n",
    "\n",
    "      train_loss, val_loss = train_epoch(train_loader,val_loader, test_loader,\n",
    "                                         encoder, decoder,\n",
    "                                         encoder_optimizer, decoder_optimizer,\n",
    "                                         criterion, tf)\n",
    "\n",
    "      print('%s (%d %d%%) /// Train Loss: %.4f - Validation Loss: %.4f' % (timeSince(start, epoch / num_epoch),\n",
    "                                      epoch, epoch / num_epoch * 100, train_loss, val_loss))\n",
    "\n",
    "      if val_loss < best_val :\n",
    "        best_val = val_loss\n",
    "        patience = 0\n",
    "      else :\n",
    "        patience += 1 \n",
    "      \n",
    "      if patience > PATIENCE_THRESHOLD : \n",
    "        print(\"EARLY STOPPING !!!\")\n",
    "        plt.plot(x, train_plot, color = 'blue', label = 'Train Loss')\n",
    "        plt.plot(x, val_plot, color = 'red', label = 'Validation Loss')\n",
    "        plt.title(\"Final Plot Before Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "      # visualize(smi_list[r1], encoder, decoder, smi_dic, longest_smi, output_path=ATTENTION_IMAGE_OUTPUT_PATH, output_name=f\"R1-E{epoch}\")\n",
    "      # visualize(smi_list[r2], encoder, decoder, smi_dic, longest_smi, output_path=ATTENTION_IMAGE_OUTPUT_PATH, output_name=f\"R2-E{epoch}\")\n",
    "      # visualize(smi_list[r3], encoder, decoder, smi_dic, longest_smi, output_path=ATTENTION_IMAGE_OUTPUT_PATH, output_name=f\"R3-E{epoch}\")\n",
    "      \n",
    "\n",
    "      train_plot.append(train_loss), val_plot.append(val_loss)\n",
    "      x = np.linspace(0, num_epoch, epoch)\n",
    "      if epoch == 1 : \n",
    "        continue\n",
    "      if epoch % 5 == 0 :\n",
    "        plt.plot(x, train_plot, color = 'blue', label = 'Train Loss')\n",
    "        plt.plot(x, val_plot, color = 'red', label = 'Validation Loss')\n",
    "        plt.title(f'Epoch {epoch}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(dim_model=DIM_MODEL,\n",
    "                  num_block=NUM_BLOCK,\n",
    "                  num_head=NUM_HEAD,\n",
    "                  dropout=DROPOUT,\n",
    "                  fe = FE,\n",
    "                  len_dic=len(smi_dic)).to(device)\n",
    "\n",
    "decoder = Decoder(dim_model=DIM_MODEL,\n",
    "                  num_block=NUM_BLOCK,\n",
    "                  num_head=NUM_HEAD,\n",
    "                  dropout=DROPOUT,\n",
    "                  fe=FE,\n",
    "                  longest_coor=longest_coor,\n",
    "                  ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtf_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTEACHER_FORCING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 72\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, val_loader, test_loader, encoder, decoder, num_epoch, learning_rate, tf_rate)\u001b[0m\n\u001b[0;32m     69\u001b[0m encoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     70\u001b[0m decoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 72\u001b[0m train_loss, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m) /// Train loss: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m - Test loss: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (timeSince(start, epoch \u001b[38;5;241m/\u001b[39m num_epoch),\n\u001b[0;32m     78\u001b[0m                                 epoch, epoch \u001b[38;5;241m/\u001b[39m num_epoch \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, train_loss, val_loss))\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val :\n",
      "Cell \u001b[1;32mIn[10], line 27\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(train_loader, val_loader, test_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, tf)\u001b[0m\n\u001b[0;32m     23\u001b[0m   prediction, cross_attn \u001b[38;5;241m=\u001b[39m decoder(e_all, e_last)\n\u001b[0;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(prediction, target)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep(), decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     31\u001b[0m epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_loader,val_loader, test_loader, encoder, decoder,\n",
    "      num_epoch=NUM_EPOCHS,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      tf_rate = TEACHER_FORCING_RATE,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1836332423346385\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.L1Loss()\n",
    "test_loss = 0\n",
    "for input, target in test_loader :\n",
    "    prediction, _, _ = evaluate(input, encoder, decoder, smi_dic, longest_smi)\n",
    "    loss = criterion(prediction, target)\n",
    "    test_loss += loss.item()\n",
    "\n",
    "print(test_loss / len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
