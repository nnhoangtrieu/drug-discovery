{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_atom(smi_path) :\n",
    "    smi_dic = {}\n",
    "    i = 0\n",
    "\n",
    "    with open(smi_path, 'r') as file :\n",
    "        for smi in file :\n",
    "            smi = rdkit.Chem.MolFromSmiles(smi)\n",
    "            for atom in smi.GetAtoms() :\n",
    "                atom = atom.GetSymbol()\n",
    "                if atom not in smi_dic : \n",
    "                    smi_dic[atom] = i \n",
    "                    i += 1\n",
    "    return smi_dic\n",
    "\n",
    "def replace_atom(smi_list) :\n",
    "    smi_list = [smi.replace('Cl', 'X')\n",
    "                   .replace('Br', 'Y')\n",
    "                   .replace('Na', 'Z')\n",
    "                   .replace('Ba', 'T') for smi in smi_list]\n",
    "    return smi_list\n",
    "\n",
    "def get_longest(input_list) :\n",
    "    longest = 0\n",
    "    for i in input_list :\n",
    "        if len(i) > longest :\n",
    "            longest = len(i)\n",
    "    return longest\n",
    "\n",
    "def get_coor(coor_path) :\n",
    "    coor_list = []\n",
    "    supplier = rdkit.Chem.SDMolSupplier(coor_path)\n",
    "    for mol in supplier:\n",
    "        coor = []\n",
    "        if mol is not None:\n",
    "            conformer = mol.GetConformer()\n",
    "            for atom in mol.GetAtoms():\n",
    "                atom_idx = atom.GetIdx()\n",
    "                x, y, z = conformer.GetAtomPosition(atom_idx)\n",
    "                coor_atom = list((x,y,z))\n",
    "                coor.append(coor_atom)\n",
    "        coor_list.append(coor)\n",
    "\n",
    "    # Replace invalid idx\n",
    "    for i, coor in enumerate(coor_list):\n",
    "        \n",
    "        if len(coor) == 0 :\n",
    "            if i == 0 :\n",
    "                coor_list = coor_list[1:]\n",
    "            coor_list[i] = coor_list[i-1]\n",
    "    return coor_list\n",
    "\n",
    "def get_xyz(coor_list) :\n",
    "    X, Y, Z = [], [], []\n",
    "    for coor in coor_list :\n",
    "        x_list, y_list, z_list = [], [], []\n",
    "        for x, y, z in coor :\n",
    "            x_list.append(x)\n",
    "            y_list.append(y)\n",
    "            z_list.append(z)\n",
    "        X.append(x_list)\n",
    "        Y.append(y_list)\n",
    "        Z.append(z_list)\n",
    "    return  X, Y, Z\n",
    "\n",
    "def get_smi(smi_path) :\n",
    "    smi_list = []\n",
    "    i = 0\n",
    "    with open(smi_path, 'r') as file :\n",
    "        for smi in file :\n",
    "            if rdkit.Chem.MolFromSmiles(smi) is None :\n",
    "                if len(smi_list) == 0 :\n",
    "                    continue \n",
    "                smi_list.append(smi_list[i-1])\n",
    "                i += 1 \n",
    "                continue \n",
    "            smi_list.append(smi)\n",
    "            i += 1\n",
    "    \n",
    "    smi_list = [smi[:-1] for smi in smi_list]\n",
    "    smi_list = [smi + 'E' for smi in smi_list]\n",
    "    smi_list = replace_atom(smi_list)\n",
    "    return smi_list\n",
    "\n",
    "def get_dic(smi_list) :\n",
    "    smi_dic = {'x': 0,\n",
    "               'E': 1}\n",
    "    i = len(smi_dic)\n",
    "\n",
    "    for smi in smi_list : \n",
    "        for atom in smi :\n",
    "            if atom not in smi_dic : \n",
    "                smi_dic[atom] = i\n",
    "                i += 1 \n",
    "    return smi_dic \n",
    "\n",
    "def smi2int(smi, smi_dic, longest_smi) :\n",
    "    smi = list(smi)\n",
    "    smint = [smi_dic[atom] for atom in smi]\n",
    "    smint = smint + [0] * (longest_smi - len(smint))\n",
    "    return smint \n",
    "\n",
    "def normalize_coor(coor_list) :\n",
    "    n_coor_list = []\n",
    "\n",
    "    for mol_coor in coor_list :\n",
    "        n_mol_coor = []\n",
    "\n",
    "        x_origin, y_origin, z_origin = mol_coor[0]\n",
    "\n",
    "        for atom_coor in mol_coor :\n",
    "            n_atom_coor = [round(atom_coor[0] - x_origin, 2), \n",
    "                        round(atom_coor[1] - y_origin, 2), \n",
    "                        round(atom_coor[2] - z_origin, 2)]\n",
    "            n_mol_coor.append(n_atom_coor)\n",
    "        n_coor_list.append(n_mol_coor)\n",
    "    return n_coor_list\n",
    "\n",
    "def pad_coor(coor_list, longest_coor) :\n",
    "    p_coor_list = []\n",
    "\n",
    "    for i in coor_list :\n",
    "        if len(i) < longest_coor :\n",
    "            zeros = [[0,0,0]] * (longest_coor - len(i))\n",
    "            zeros = torch.tensor(zeros)\n",
    "            i = torch.tensor(i)\n",
    "            i = torch.cat((i, zeros), dim = 0)\n",
    "            p_coor_list.append(i)\n",
    "        else :\n",
    "            p_coor_list.append(i)\n",
    "    return p_coor_list\n",
    "\n",
    "def train_test_split(input, ratio = 0.9) :\n",
    "    ratio = int(len(input) * ratio) \n",
    "\n",
    "    train = input[:ratio]\n",
    "    test = input[ratio:]\n",
    "    return train, test\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEncoderDataset(Dataset) :\n",
    "    def __init__(self, x, y, z, smint) :\n",
    "        self.x = torch.tensor(x, device=device)\n",
    "        self.y = torch.tensor(y, device=device)\n",
    "        self.z = torch.tensor(z, device=device)\n",
    "        self.smint = torch.tensor(smint, dtype=torch.long, device=device)\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return len(self.smint)\n",
    "\n",
    "    def __getitem__(self, idx) :\n",
    "        return self.x[idx], self.y[idx], self.z[idx], self.smint[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smi_list = get_smi('../data/ADAGRASIB_UNIQUE_SMILES.txt')\n",
    "coor_list = get_coor('../data/ADAGRASIB_COOR.sdf')\n",
    "smi_dic = get_dic(smi_list)\n",
    "longest_smi, longest_coor = get_longest(smi_list), get_longest(coor_list)\n",
    "smint_list = [smi2int(smi, smi_dic, longest_smi) for smi in smi_list]\n",
    "np_coor_list = pad_coor(normalize_coor(coor_list), longest_coor)\n",
    "X, Y, Z = get_xyz(np_coor_list)\n",
    "train_X, test_X = train_test_split(X)\n",
    "train_Y, test_Y = train_test_split(Y)\n",
    "train_Z, test_Z = train_test_split(Z)\n",
    "train_smint, test_smint = train_test_split(smint_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 16\n",
    "train_set = MultiEncoderDataset(train_X, train_Y, train_Z, train_smint)\n",
    "test_set = MultiEncoderDataset(test_X, test_Y, test_Z, test_smint)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=B, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Attention(nn.Module): # Neural Network Attention \n",
    "    def __init__(self, dim_model):\n",
    "        super(NN_Attention, self).__init__()\n",
    "        self.Wa = nn.Linear(dim_model, dim_model)\n",
    "        self.Ua = nn.Linear(dim_model, dim_model)\n",
    "        self.Va = nn.Linear(dim_model, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        \n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights \n",
    "    \n",
    "\n",
    "class DP_Attention(nn.Module) : # Dot Product Attention\n",
    "    def __init__(self, dim_model, num_head) :\n",
    "        super(DP_Attention, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.dim_head = dim_model // num_head\n",
    "\n",
    "        self.Q = nn.Linear(dim_model, dim_model)\n",
    "        self.K = nn.Linear(dim_model, dim_model)\n",
    "        self.V = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "        self.out = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, Q, K, V) :\n",
    "        B = Q.size(0) # Shape Q, K, V: (B, longest_smi, dim_model)\n",
    "\n",
    "        Q, K, V = self.Q(Q), self.K(K), self.V(V)\n",
    "\n",
    "        len_Q, len_K, len_V = Q.size(1), K.size(1), V.size(1)\n",
    "\n",
    "        Q = Q.reshape(B, self.num_head, len_Q, self.dim_head)\n",
    "        K = K.reshape(B, self.num_head, len_K, self.dim_head)\n",
    "        V = V.reshape(B, self.num_head, len_V, self.dim_head)\n",
    "        \n",
    "        K_T = K.transpose(2,3).contiguous()\n",
    "\n",
    "        attn_score = Q @ K_T\n",
    "\n",
    "        attn_score = attn_score / (self.dim_head ** 1/2)\n",
    "\n",
    "        attn_distribution = torch.softmax(attn_score, dim = -1)\n",
    "\n",
    "        attn = attn_distribution @ V\n",
    "\n",
    "        attn = attn.reshape(B, len_Q, self.num_head * self.dim_head)\n",
    "        \n",
    "        attn = self.out(attn)\n",
    "\n",
    "        return attn, attn_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head, dropout) :\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.Self_Attention = DP_Attention(dim_model, num_head)\n",
    "        self.GRU = nn.GRU(2 * dim_model, dim_model, batch_first=True)\n",
    "        self.Up_Size = nn.Linear(1, dim_model)\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "        self.LayerNorm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        x = x.unsqueeze(-1)\n",
    "\n",
    "        x = self.Dropout(self.Up_Size(x))\n",
    "\n",
    "        x = self.LayerNorm(x) \n",
    "\n",
    "        attn, self_attn = self.Self_Attention(x, x, x) \n",
    "\n",
    "        input_gru = torch.cat((attn, x), dim = -1) \n",
    "\n",
    "        e_all, e_last = self.GRU(input_gru)\n",
    "        \n",
    "        return e_all, e_last, self_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) : \n",
    "    def __init__(self, dim_model, output_size, longest_smi, dropout) :\n",
    "        super(Decoder, self).__init__()\n",
    "        self.longest_smi = longest_smi\n",
    "        self.Embedding = nn.Embedding(longest_smi, dim_model)\n",
    "\n",
    "        self.xCross_Attention = NN_Attention(dim_model) \n",
    "        self.yCross_Attention = NN_Attention(dim_model) \n",
    "        self.zCross_Attention = NN_Attention(dim_model) \n",
    "\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "        self.GRU = nn.GRU(dim_model,dim_model, batch_first=True)\n",
    "        self.Linear = nn.Linear(dim_model, output_size)\n",
    "\n",
    "        self.LayerNorm1 = nn.LayerNorm(dim_model)\n",
    "        self.LayerNorm2 = nn.LayerNorm(dim_model)\n",
    "\n",
    "\n",
    "    def forward(self, xe_all, ye_all, ze_all, xe_last, ye_last, ze_last, target = None) :\n",
    "        B = xe_all.size(0)\n",
    "\n",
    "        d_input = torch.zeros(B, 1, dtype = torch.long, device = device)\n",
    "\n",
    "        xdh = xe_last\n",
    "        ydh = ye_last\n",
    "        zdh = ze_last\n",
    "        dh = None \n",
    "\n",
    "        outputs, cross_attn = [], [] \n",
    "        for i in range(self.longest_smi) :\n",
    "            output, dh, step_attn = self.forward_step(d_input, xdh,ydh,zdh, xe_all, ye_all, ze_all, dh)\n",
    "\n",
    "            outputs.append(output), cross_attn.append(step_attn)\n",
    "\n",
    "            if target is not None :\n",
    "                d_input = target[:, i].unsqueeze(1)\n",
    "            else :\n",
    "                _, topi = output.topk(1)\n",
    "                d_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        outputs = torch.cat(outputs, dim = 1)\n",
    "        outputs = F.log_softmax(outputs, dim = -1)\n",
    "\n",
    "        # cross_attn = torch.cat(cross_attn, dim = 1)\n",
    "\n",
    "        return outputs, cross_attn\n",
    "    \n",
    "    def forward_step(self, d_input, xdh, ydh, zdh, xe_all, ye_all, ze_all, dh) :\n",
    "        embedded = self.Dropout(self.Embedding(d_input))\n",
    "\n",
    "        x_Q = xdh.permute(1, 0, 2)\n",
    "        y_Q = ydh.permute(1, 0, 2)\n",
    "        z_Q = zdh.permute(1, 0, 2)\n",
    "\n",
    "        x_attn, x_cross_attn = self.xCross_Attention(x_Q, xe_all)\n",
    "        y_attn, y_cross_attn = self.yCross_Attention(y_Q, ye_all)\n",
    "        z_attn, z_cross_attn = self.zCross_Attention(z_Q, ze_all)\n",
    "\n",
    "        input_gru = embedded + x_attn + y_attn + z_attn \n",
    "\n",
    "        # input_gru = self.LayerNorm(input_gru)\n",
    "\n",
    "        if dh is None :\n",
    "            output, dh = self.GRU(input_gru, xdh + ydh + zdh)\n",
    "        else : \n",
    "            output, dh = self.GRU(input_gru, dh) \n",
    "\n",
    "        output = self.Linear(output) \n",
    "\n",
    "        return output, dh, (x_cross_attn, y_cross_attn, z_cross_attn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_loader,test_loader,\n",
    "                xencoder, yencoder, zencoder, decoder,\n",
    "                xencoder_optimizer, yencoder_optimizer, zencoder_optimizer, decoder_optimizer,\n",
    "                criterion, tf):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_test_loss = 0\n",
    "\n",
    "    for x, y, z, target in train_loader:\n",
    "        xencoder_optimizer.zero_grad(), yencoder_optimizer.zero_grad(), zencoder_optimizer.zero_grad(), decoder_optimizer.zero_grad()\n",
    "        \n",
    "        xe_all, xe_last, _ = xencoder(x)\n",
    "        ye_all, ye_last, _ = yencoder(y)\n",
    "        ze_all, ze_last, _ = zencoder(z)\n",
    "\n",
    "        # Teacher Forcing\n",
    "        if tf :\n",
    "          prediction, _ = decoder(xe_all, ye_all, ze_all, xe_last, ye_last, ze_last, target)\n",
    "        else :\n",
    "          prediction, _ = decoder(xe_all, ye_all, ze_all, xe_last, ye_last, ze_last)\n",
    "\n",
    "        print(f'prediction: {prediction.shape}')\n",
    "        print(f'target: {target.shape}')\n",
    "        loss = criterion(\n",
    "           prediction.view(-1, prediction.size(-1)),\n",
    "           target.view(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        xencoder_optimizer.step(), yencoder_optimizer.step(), zencoder_optimizer.step(), decoder_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "    xencoder.eval(), yencoder.eval(), zencoder.eval(), decoder.eval()\n",
    "\n",
    "    with torch.no_grad() :\n",
    "      for x, y , z, target in test_loader :\n",
    "        xe_all, xe_last, _ = xencoder(x)\n",
    "        ye_all, ye_last, _ = yencoder(y)\n",
    "        ze_all, ze_last, _ = zencoder(z)\n",
    "        prediction, cross_attn = decoder(xe_all, ye_all, ze_all, xe_last, ye_last, ze_last)\n",
    "\n",
    "        test_loss = criterion(\n",
    "           prediction.view(-1, prediction.size(-1)),\n",
    "           target.view(-1)\n",
    "        )\n",
    "        total_test_loss += test_loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader), total_test_loss / len(test_loader)\n",
    "\n",
    "def train(train_loader, test_loader, xencoder, yencoder, zencoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=1, visual_path= \"\", tf_rate = 1):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss_total = 0  \n",
    "    test_loss_total = 0\n",
    "\n",
    "    xencoder_optimizer = torch.optim.Adam(xencoder.parameters(), lr=learning_rate)\n",
    "    yencoder_optimizer = torch.optim.Adam(yencoder.parameters(), lr=learning_rate)\n",
    "    zencoder_optimizer = torch.optim.Adam(zencoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    tf = True\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "      if epoch > (tf_rate * n_epochs) :\n",
    "        tf = False\n",
    "      xencoder.train()\n",
    "      yencoder.train()\n",
    "      zencoder.train()\n",
    "      decoder.train()\n",
    "\n",
    "      train_loss, test_loss = train_epoch(train_loader, test_loader,\n",
    "                                          xencoder, yencoder, zencoder, decoder,\n",
    "                                          xencoder_optimizer, yencoder_optimizer,zencoder_optimizer, decoder_optimizer,\n",
    "                                          criterion, tf)\n",
    "      train_loss_total += train_loss\n",
    "      test_loss_total += test_loss\n",
    "\n",
    "      if epoch % print_every == 0:\n",
    "          train_loss_avg = train_loss_total / print_every\n",
    "          test_loss_avg = test_loss_total / print_every\n",
    "          train_loss_total = 0\n",
    "          test_loss_total = 0\n",
    "          print('%s (%d %d%%) /// Train loss: %.4f - Test loss: %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                      epoch, epoch / n_epochs * 100, train_loss_avg, test_loss_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_MODEL = 256\n",
    "NUM_HEAD = 4 \n",
    "DROPOUT = 0.5 \n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "TF_RATE = 0.0\n",
    "\n",
    "xencoder = Encoder(dim_model=DIM_MODEL, num_head=NUM_HEAD, dropout=DROPOUT)\n",
    "yencoder = Encoder(dim_model=DIM_MODEL, num_head=NUM_HEAD, dropout=DROPOUT)\n",
    "zencoder = Encoder(dim_model=DIM_MODEL, num_head=NUM_HEAD, dropout=DROPOUT)\n",
    "\n",
    "decoder = Decoder(dim_model=DIM_MODEL, output_size=len(smi_dic), longest_smi=longest_smi, dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18,  2,  6,  7,  2,  2,  7,  2,  8,  2,  7,  2,  6,  9,  3, 13,  5,  8,\n",
      "          7, 12,  9,  8,  7, 12,  9,  4,  2,  2,  7,  2,  1,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  2,  4,  8,  2,  2,  9,  2,  2,  4,  2,  8,  7, 12,  9,  2,  8,  7,\n",
      "         12,  9,  4,  2,  6,  7,  2,  8, 18,  9,  2,  7,  2,  2,  7,  2,  6,  1,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  2, 12,  2,  6,  7,  2,  2,  7,  2,  8,  2,  7,  2,  6,  9,  2, 11,\n",
      "          7,  4,  4,  7,  2, 24,  2,  7,  2,  2,  8,  7,  4,  3,  4,  5, 11, 24,\n",
      "          9, 13,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  2,  6,  7,  2,  2,  7,  2,  8,  2,  7,  2,  6,  9,  3,  4,  5, 11,\n",
      "          4,  7,  2,  2, 24,  7,  2, 11,  4,  7,  2,  8, 13,  9,  4,  2, 24,  7,\n",
      "         12,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  2,  8, 12,  9,  2,  4,  2,  8,  7, 12,  9,  2,  8,  7, 12,  9,  4,\n",
      "          2,  2,  6,  7,  2,  2,  7,  2,  8, 14,  9,  2,  7,  2,  6,  1,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 12,  2,  2,  4,  2,  8,  7, 12,  9,  2,  8,  7, 12,  9,  4,  2,  2,\n",
      "          2,  6,  7,  2,  2,  7,  2,  8,  2,  7,  2,  6,  9,  4,  8,  2,  9,  2,\n",
      "          1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  2,  8,  7,  2, 17,  2,  6,  7,  2,  2,  7,  2,  2,  7,  2,  6,  9,\n",
      "         15,  2,  8,  7, 12,  9,  4, 11,  2,  2, 24,  2,  2, 11,  2, 13, 24,  1,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [18,  2,  6,  7,  2,  8, 13,  2,  8,  7, 12,  9,  4,  6,  2,  2, 11,  7,\n",
      "          2,  2,  7,  2,  2,  7,  2, 11,  9,  2,  7, 12,  1,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [18, 23,  2,  2,  6,  2,  2,  4,  8,  2,  2,  6,  9,  2,  2,  8, 12,  9,\n",
      "          2, 12,  2,  8,  2,  9,  8,  2,  9,  2,  7,  2,  1,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  2,  8,  7, 12,  9,  2,  8,  7, 12,  9,  4,  2,  2,  8,  4,  6,  2,\n",
      "          2, 13,  2,  2,  6,  9,  2, 11,  7,  2,  2,  7,  2, 12, 11,  1,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  7,  2,  6,  4,  8,  2,  4,  2, 11,  7,  4,  2,  7,  2,  2,  7,  4,\n",
      "         11,  9,  2, 24,  7,  2,  8,  2,  7,  2,  2,  7,  2, 24,  9,  2,  6,  7,\n",
      "         12,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 12,  2,  6,  7,  2,  2,  7,  2,  8,  2,  7,  2,  6,  9,  4, 11,  2,\n",
      "          8,  7, 12,  9,  2,  7,  2,  2, 11,  7, 12,  1,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  2,  8,  7, 12,  9, 17,  2,  7,  2, 15,  2,  8,  7, 12,  9,  4,  2,\n",
      "          6,  7,  2,  8, 14,  9,  2,  7,  2,  2,  7,  2,  6,  1,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 12,  2,  8,  7, 12,  9,  2,  8,  7, 12,  9,  2,  2,  8,  7, 12,  9,\n",
      "          2,  6,  7,  2,  2,  7,  2,  2,  7,  2,  6,  1,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  7,  2,  8,  4,  2,  6,  7,  4,  4,  7,  2, 13,  6,  9, 17,  2,  7,\n",
      "          2, 17,  2, 11,  7,  2,  2,  7,  2, 12, 11,  1,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  7,  2,  8,  4,  2,  2,  2, 16,  2,  9,  2,  6,  7,  2,  2,  8,  7,\n",
      "          4,  2,  7,  2,  6,  9, 12,  2, 11,  2,  2,  2,  2, 11,  1,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "for x, y, z, smint in train_loader :\n",
    "    print(smint)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: torch.Size([16, 47, 30])\n",
      "target: torch.Size([16, 47])\n",
      "prediction: torch.Size([16, 47, 30])\n",
      "target: torch.Size([16, 47])\n",
      "prediction: torch.Size([16, 47, 30])\n",
      "target: torch.Size([16, 47])\n",
      "prediction: torch.Size([16, 47, 30])\n",
      "target: torch.Size([16, 47])\n",
      "prediction: torch.Size([16, 47, 30])\n",
      "target: torch.Size([16, 47])\n",
      "prediction: torch.Size([16, 47, 30])\n",
      "target: torch.Size([16, 47])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m      \u001b[49m\u001b[43mxencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m      \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTF_RATE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 77\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, test_loader, xencoder, yencoder, zencoder, decoder, n_epochs, learning_rate, print_every, visual_path, tf_rate)\u001b[0m\n\u001b[0;32m     74\u001b[0m zencoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     75\u001b[0m decoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 77\u001b[0m train_loss, test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mxencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mxencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mzencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m train_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\n\u001b[0;32m     82\u001b[0m test_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m test_loss\n",
      "Cell \u001b[1;32mIn[9], line 29\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(train_loader, test_loader, xencoder, yencoder, zencoder, decoder, xencoder_optimizer, yencoder_optimizer, zencoder_optimizer, decoder_optimizer, criterion, tf)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[0;32m     25\u001b[0m    prediction\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, prediction\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[0;32m     26\u001b[0m    target\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     27\u001b[0m )\n\u001b[1;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m xencoder_optimizer\u001b[38;5;241m.\u001b[39mstep(), yencoder_optimizer\u001b[38;5;241m.\u001b[39mstep(), zencoder_optimizer\u001b[38;5;241m.\u001b[39mstep(), decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     33\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_loader, test_loader,\n",
    "      xencoder, yencoder, zencoder, decoder,\n",
    "      n_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, tf_rate=TF_RATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
